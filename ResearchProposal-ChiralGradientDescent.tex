\documentclass[12pt, a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry} % Standard margin setup
\usepackage{amsmath, amsfonts, amssymb} % For math
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{graphicx} % For images
\usepackage{hyperref} % For hyperlinks
\usepackage{enumitem} % For better list control
\usepackage{abstract} % For abstract formatting
\usepackage{titlesec} % For title formatting
\usepackage{cite}

% Title formatting
\titleformat{\section}{\normalfont\Large\bfseries}{\thesection}{1em}{}
\titleformat{\subsection}{\normalfont\large\bfseries}{\thesubsection}{1em}{}

% Spacing
\setlength{\parskip}{1em}
\setlength{\parindent}{0em}

% Abstract spacing
\setlength{\absleftindent}{0mm}
\setlength{\absrightindent}{0mm}

% Title and author information
\title{\vspace{-2cm}\textbf{Research Proposal: Exploring Chiral Topologies for Enhanced Gradient Descent}}
\author{\textbf{Paul Lowndes} \\ \texttt{ZeroTrust@NSHkr.com}}
\date{\small December 4, 2024} % Smaller date formatting

\begin{document}

\maketitle
\vspace{-1.5em} % Adjust spacing after title

\begin{abstract}
This research proposal outlines a novel approach to gradient descent optimization, termed Chiral Gradient Descent (CGD), which incorporates topological information and rotational dynamics inspired by chirality in biological systems. The proposed methodology will investigate how chiral structures within neural networks can be leveraged to improve exploration of the parameter space, potentially leading to more robust and efficient training. The research will involve developing a mathematical framework for CGD, implementing the algorithm in a deep learning framework, and conducting experiments on benchmark datasets to evaluate CGD's performance compared to standard gradient descent methods. The expected outcomes include a refined mathematical formulation of CGD, an open-source implementation, and experimental results demonstrating the potential benefits of CGD for various machine learning tasks.
\end{abstract}

\section{Introduction}

Gradient descent is widely used for optimizing machine learning models, but its susceptibility to local minima remains a challenge. Chiral gradient descent (CGD) presents a novel approach, drawing inspiration from the concept of chirality. CGD aims to enhance standard gradient descent by introducing asymmetry into the update process, motivated by the observation that many biological systems, including neural networks in the brain, exhibit chiral structures. We hypothesize this could lead to more efficient exploration of the parameter space and faster, more stable convergence.

Gradient descent, while highly effective, suffers from limitations in complex landscapes, thus the exploration of alternative methods. Chiral Gradient Descent (CGD) introduces a novel mechanism by leveraging chiral asymmetry, inspired by natural systems where chirality plays a crucial role in dynamic interactions. By incorporating topological information and rotational dynamics, CGD aims to enhance the exploration capabilities of traditional gradient descent, potentially leading to faster and more robust convergence.  This builds on previous work exploring various gradient descent variants \cite{induraj2023variants}.

\section{Chiral Gradient Descent}

Chiral Gradient Descent (CGD) modifies the gradient update rule by incorporating chiral vectors, which introduce rotational dynamics into the optimization process. This approach is inspired by natural asymmetry observed in biological systems. The chiral term, incorporating a sigmoid function, allows for dynamic adjustments based on topological distances within the network, potentially enhancing exploration of the parameter space and leading to more robust convergence. This sigmoid function modulates the influence of each chiral pair based on the topological distance between neurons, allowing for local chiral effects to dominate while diminishing the impact of distant pairs. The mathematical formulation of CGD involves the cross product of the gradient with a chiral vector, adding a layer of complexity and potential to the optimization process.


\section{Applying Chiral Topologies}

We represent a neural network's topology as a graph \(G = (V, E)\). A chiral pair of neurons \((v_i, v_j)\) is defined based on topological asymmetry, such as differences in shortest path lengths from a common ancestor. For each chiral pair, a chiral vector \(\mathbf{c}_{ij}\) is defined, reflecting their topological relationship. Measures such as differences in path lengths, or other metrics relating to properties of chiral topologies will determine a numerical representation for relative asymmetry and help refine our distance metric between them. These metrics could involve other topological or information-theoretic properties like local or global curvature or the orientation or density of node or edge data.

\section{Mathematical Formulation}

The core innovation of CGD lies in its gradient calculation which incorporates chiral vectors:

\begin{equation} \label{eq:cgd_sigmoid}
\boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}_t - \alpha \nabla L(\boldsymbol{\theta}_t) + \beta \sum_{i,j \in C} s(w_{ij}, \mathbf{c}_{ij}) (\nabla L(\boldsymbol{\theta}_t) \times \mathbf{c}_{ij})
\end{equation}

Where:
\begin{itemize}
    \item \(\boldsymbol{\theta}_t\) represents the parameter vector at iteration \(t\).
    \item \(\alpha\) denotes the learning rate.
    \item \(\nabla L(\boldsymbol{\theta}_t)\) is the gradient of the loss function at iteration \(t\).
    \item \(\beta\) represents the chirality parameter, which modulates the influence of the chiral vectors.
    \item \(C\) denotes the set of chiral pairs being considered during the update step, which may vary at each iteration depending on the method or constraints being used by the researcher.
    \item \(w_{ij}\) represents a weight associated with the chiral pair \((i,j)\), and may reflect asymmetry measures related to properties of their chiral topologies.
    \item \(\mathbf{c}_{ij}\) represents the chiral vector for the chiral pair \((i, j)\).
    \item \(\times\) represents the cross-product.
    \item \(s(w_{ij},\mathbf{c}_{ij})\) is a function designed to blend the chiral vector's influence with considerations based on a weight \(w_{ij}\).
\end{itemize}

\section{Training Intuition and Higher Dimensions}
Understanding chirality in higher dimensions can begin with visualizing simple cases (2D and 3D), followed by mathematical generalization to 4D, 5D, and beyond. Training involves visualization, representation with vectors and matrices, implementing simple transformations, and generalizing to higher dimensions.

Understanding the role of chirality in higher dimensions requires a shift in perspective, as traditional geometric intuitions may not directly apply. By visualizing lower-dimensional cases and gradually extending these insights to higher dimensions, researchers can develop a deeper intuition for the impact of chiral dynamics on learning. The use of vector and matrix representations allows for the implementation of simple transformations that can be generalized, providing a framework for exploring the effects of chirality across various dimensional spaces.





\section{Towards CGD: A Synthesis}

Chiral Gradient Descent (CGD) aims to enhance the efficiency and robustness of standard gradient descent by incorporating chirality—a concept of asymmetry—into the optimization process. This asymmetry is inspired by the prevalence of chiral structures and functions in biological systems, suggesting that introducing similar principles in optimization algorithms could lead to advantages not seen in more traditional, gradient-based methods.

\subsection{The Chiral Update Rule}

The core innovation of CGD is its update rule:

\begin{equation} \label{eq:cgd_sigmoid_final}
\boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}t - \alpha \nabla L(\boldsymbol{\theta}t) + \beta \sum{i,j \in C(\boldsymbol{\theta}t)} \frac{| \mathbf{c}{ij} |}{1 + e^{-\gamma d{ij}}} (\nabla L(\boldsymbol{\theta}t) \times \mathbf{c}{ij})
\end{equation}

Where:




\begin{itemize}
    \item \(\boldsymbol{\theta}_t\): Parameter vector at time \(t\).
    \item \(\alpha\): Learning rate.
    \item \(\nabla L(\boldsymbol{\theta}_t)\): Gradient of the loss function.
    \item \(\beta\): Global chirality parameter.
    \item \(C(\boldsymbol{\theta}_t)\): Set of relevant chiral pairs, potentially changing dynamically during training.
    \item \(\mathbf{c}_{ij}\): Chiral vector for pair (i, j), representing the direction and magnitude of the chiral influence (calculated using methods detailed in Section 5).
    \item \(d_{ij}\): Topological distance between nodes i and j, reflecting their structural relationship. This could be shortest-path distance, a measure of graph centrality, or other relevant metrics, depending on the properties of the data.
    \item \(\gamma\): Parameter controlling the sigmoid function’s steepness, determining the sensitivity to topological distance. Larger values of \(\gamma\) result in a sharper transition in the sigmoid function, while smaller values result in a more gradual transition, providing a mechanism to adjust the influence of distance on the weights.
\end{itemize}

\subsection{Dynamic Chiral Pair Selection: ($C(\boldsymbol{\theta}_t))$}

The set of relevant chiral pairs, ($C(\boldsymbol{\theta}_t)$), is not static but dynamically determined at each iteration t. This selection process can be based on several factors:

\begin{itemize}
\item \textbf{Gradient Magnitude:} Prioritize pairs whose corresponding gradients exceed a certain threshold, focusing on areas of the network where learning is most active.
\item \textbf{Topological Distance:} Include pairs within a certain topological radius, preventing long-range chiral interactions from overwhelming the update.
\item \textbf{Asymmetry Score:} Incorporate an asymmetry score (e.g., based on the cosine similarity between the feature embeddings of the chiral pair, as discussed in Section 5). Select pairs whose asymmetry scores exceed a specific threshold, focusing on the most significant asymmetries within the network structure.
\item \textbf{Temporal Dynamics:} For recurrent networks, introduce a temporal component into the selection process, considering factors like previous activation patterns or temporal correlations.
\end{itemize}

\subsection{Biological Inspiration}

The sigmoid function in Equation \ref{eq:cgd_sigmoid_final} is inspired by the graded nature of synaptic weights in biological neural systems. The weight ($w_{ij}$) can be interpreted as reflecting the strength of the chiral interaction, analogous to synaptic efficacy. The sigmoid function ensures that the chiral term's influence decreases smoothly with increasing distance, mirroring how the influence of a neuron on its neighbors diminishes with physical distance in biological circuits.






 
\section{Identifying Chiral Pairs: A Topologically-Informed Approach}

This section details a novel method for identifying chiral pairs within complex networks, extending the approach described in Zhang et al. \cite{zhang2018machine} for identifying topological invariants.  Instead of directly predicting topological invariants, we adapt their convolutional neural network (CNN) architecture to identify pairs of nodes exhibiting chiral topological features, focusing on asymmetries within the network's structure and information flow.  This will form Phase 1 of our system for identifying chiral pairs to be used in subsequent phases to implement chiral gradient descent.

\subsection{Network Representation}

As in the previous sections, we represent networks as directed graphs $G = (V, E)$, where $V$ is the set of nodes and $E$ is the set of directed edges. Each edge $e_{ij} \in E$ connecting node $v_i$ to node $v_{j}$ has an associated weight $w_{ij}$ representing the strength of the connection (e.g., correlation between node activations, information flow, or interaction strength). We extend the graph representation by including additional node attributes that might influence the identification of chiral pairs, such as node centrality, community membership, and other topological measures that could prove useful in identifying pairs in the network.  This extended representation is richer and more nuanced compared to simple directed graphs and is necessary to capture the more complex relationships between nodes in the network.


As in the previous sections, we represent networks as directed graphs $G = (V, E)$, where $V$ is the set of nodes and $E$ is the set of directed edges.  Each edge $e_{ij} \in E$ connecting node $v_{i}$ to node $v$.

\subsection{Adapting the Convolutional Neural Network}

We adapt the CNN architecture proposed in Zhang et al. \cite{zhang2018machine}  (see Figure 1 in the original paper) to learn local topological features related to chirality. The input to the CNN will be a matrix representation of the local neighborhood around each node in the graph.  This representation will be constructed by including several elements:

Node Attributes:  Include node attributes such as centrality and community membership in the input matrix.
Edge Weights:  The edge weights from the node to its neighbors are added to the input matrix.
Shortest Path Lengths:  Compute the shortest path lengths between each pair of neighbors, which will inform the computation of the chiral vector in the subsequent phases.

This extended input representation captures both local topology and more global network features. This information is crucial for accurately identifying chiral pairs, unlike the approach in the original paper which only focused on computing the winding number.

\subsection{Chiral Pair Identification}

The output of the CNN is a vector that represents a topological feature embedding of the local neighborhood for each node.  We define a chiral pair as a pair of nodes whose topological feature embeddings show a high degree of asymmetry or anti-correlation. This asymmetry or anti-correlation is evaluated using a distance metric, such as cosine similarity. We select the top pairs that maximize the asymmetry as the chiral pairs relevant to performing CGD in subsequent phases.  The selection process could be made more complex to filter out pairs with specific traits or incorporate more data to reduce computational cost or improve performance.






\section{Identifying Chiral Pairs: A Topologically-Informed Approach}

This section details a method for identifying chiral pairs, building upon Zhang \textit{et al.} [1] but incorporating novel elements to capture asymmetry relevant to narrative structures and social networks. This forms Phase 1 of our system, providing the foundation for chiral gradient descent.

\subsection{Network Representation (Enhanced)}

We represent networks as directed graphs $G = (V, E)$ with weighted edges $w_{ij}$. Crucially, we augment this with \textbf{relative path information}. For each node $v_i$, we pre-compute:

\begin{itemize}
    \item \textbf{Shortest Path Lengths:} The shortest path length from $v_i$ to every other node $v_j$ (denoted $d(v_i, v_j)$).
    \item \textbf{Common Ancestors and Path Differences:} For each pair of nodes ($v_i, v_j$), we identify their common ancestors and calculate the \textit{difference} in path lengths from each common ancestor to $v_i$ and $v_j$. This captures the asymmetry in how $v_i$ and $v_j$ relate to their shared history or context.
\end{itemize}

This richer representation goes beyond simple connectivity, encoding structural asymmetries relevant to narratives (e.g., how different characters relate to key events) and social networks (e.g., differing perspectives on shared information).

\subsection{Convolutional Neural Network (Adapted)}

We adapt the CNN architecture from Zhang \textit{et al.} [1] to learn chiral features. The input to the CNN for node $v_i$ now includes:

\begin{itemize}
    \item \textbf{Local Neighborhood Structure:} Edge weights $w_{ij}$ for $v_j$ in $v_i$'s neighborhood.
    \item \textbf{Relative Path Information:} For each neighbor $v_j$, include the shortest path length $d(v_i, v_j)$ and the differences in path lengths from common ancestors.
\end{itemize}

This combined input allows the CNN to learn features sensitive to both local connectivity and global topological asymmetries.

\subsection{Chiral Pair Identification (Refined)}

Instead of simply using cosine distance, we define a more nuanced chirality score:

\begin{equation}
\text{ChiralScore}(v_i, v_j) = \text{Asymmetry}(F_i, F_j) \times \text{PathDifference}(v_i, v_j)
\end{equation}

Where:

\begin{itemize}
    \item $F_i, F_j$ are the feature embeddings from the CNN for nodes $v_i$ and $v_j$.
    \item $\text{Asymmetry}(F_i, F_j)$ measures the asymmetry between the embeddings (e.g., using cosine distance or a learned metric).
    \item $\text{PathDifference}(v_i, v_j)$ is a weighted average of the path length differences from common ancestors, emphasizing structural asymmetry.
\end{itemize}

Pairs with high $\text{ChiralScore}$ are identified as chiral pairs. This combined score captures both feature-level and structural asymmetry.

\subsection{Algorithm}

\begin{algorithm}[H]
\caption{Chiral Pair Identification}
\begin{algorithmic}[1]
\Require Graph $G=(V,E)$, CNN model, chirality threshold $\tau$
\State Pre-compute shortest path lengths and path differences for all node pairs
\State Initialize empty set of chiral pairs $C$
\For{each node $v_i \in V$}
    \State Construct input matrix $M_i$ (neighborhood, paths)
    \State $F_i \gets$ CNN($M_i$)
\EndFor
\For{each pair of nodes $(v_i, v_j) \in V \times V$}
    \State $S_{ij} \gets \text{ChiralScore}(v_i, v_j)$
    \If{$S_{ij} > \tau$}
        \State $C \gets C \cup \{(v_i, v_j)\}$
    \EndIf
\EndFor
\Return $C$
\end{algorithmic}
\end{algorithm}

\subsection{Discussion}

This method directly addresses the challenge of defining and detecting chirality relevant to narrative and social network analysis. By incorporating relative path information and a combined chirality score, it captures more nuanced asymmetries than simply comparing feature embeddings. The use of common ancestor paths adds a "historical" or "contextual" dimension to the chirality measure, which is particularly relevant for understanding how information and relationships evolve in narratives and social networks. The hierarchical application and the flexibility in defining the Asymmetry and PathDifference functions provide adaptability for various applications.






\subsection{Hierarchical Application}

To handle hierarchical structures, we apply the method recursively to sub-graphs within the network. This hierarchical application identifies chiral pairs at different scales, allowing for a more granular analysis of asymmetry in multi-level structures, which mirrors the multi-scale nature of hierarchical structures in narratives and social networks.


\subsection{Algorithm}


\begin{algorithm}[H]
\caption{Chiral Pair Identification}
\begin{algorithmic}[1]
\Require Graph $G=(V,E)$, CNN model, asymmetry threshold $\tau$
\State Initialize empty set of chiral pairs $C$
\For{each node $v_i \in V$}
    \State Construct input matrix $M_i$ for $v_i$'s neighborhood
    \State Obtain feature embedding vector $F_i$ from CNN($M_i$)
\EndFor
\For{each pair of nodes $(v_i, v_j) \in V \times V$}
    \State Compute asymmetry score $A_{ij} = d(F_i, F_j)$ (e.g., using cosine distance)
    \If{$A_{ij} > \tau$}
        \State Add $(v_i, v_j)$ to $C$
    \EndIf
\EndFor
\State \Return Set of chiral pairs $C$
\end{algorithmic}
\end{algorithm}


\subsection{Discussion}

This method builds upon the success of CNNs in learning complex patterns from local data and provides a rigorous approach to identify chiral pairs.  By incorporating topological features and hierarchical application, it addresses the complexities of analyzing asymmetries in real-world systems.  The choice of CNN architecture and asymmetry threshold requires careful evaluation and tuning for specific applications and datasets.  We will compare this approach with alternative methods for measuring graph similarity and asymmetry in our validation experiments.

 



\begin{equation} \label{eq:cgd_sigmoid_final}
\boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}_t - \alpha \nabla L(\boldsymbol{\theta}_t) + \beta \sum_{i,j \in C(\boldsymbol{\theta}_t)}  \frac{\| \mathbf{c}_{ij} \|}{1 + e^{-\gamma d_{ij}}} (\nabla L(\boldsymbol{\theta}_t) \times \mathbf{c}_{ij})
\end{equation}

Where:

\begin{itemize}
    \item \(\boldsymbol{\theta}_t\): Parameter vector at time \(t\).
    \item \(\alpha\): Learning rate.
    \item \(\nabla L(\boldsymbol{\theta}_t)\): Gradient of the loss function.
    \item \(\beta\): Global chirality parameter.
    \item \(C(\boldsymbol{\theta}_t)\): Set of relevant chiral pairs, potentially changing dynamically during training.
    \item \(\mathbf{c}_{ij}\): Chiral vector for pair (i, j), calculated based on topology.
    \item \(d_{ij}\): Topological distance between \(i\) and \(j\) based on features like difference in path lengths, curvature measures, node or edge distribution densities, etc.
    \item \(\gamma\): Scaling parameter for the sigmoid function, influencing the impact of \(d_{ij}\).
\end{itemize}

\begin{algorithm}
\caption{Chiral Gradient Descent (CGD)}
\label{alg:cgd}
\begin{algorithmic}
\Require Learning rate ($\alpha$), chirality parameter ($\beta$), scaling parameter ($\gamma$), initial parameters (\boldsymbol{\theta}0)
	\While{not converged}
	\State Compute gradient: ($\nabla L(\boldsymbol{\theta}t)$)
	\State Determine relevant chiral pairs: ($C(\boldsymbol{\theta}t)$) (using gradient magnitude, topological distance, asymmetry scores, and other factors)
	\State Calculate chiral vectors ($\mathbf{c}{ij}$) and distances ($d{ij}$) for ($(i, j) \in C(\boldsymbol{\theta}t)$)
	\State ($\Delta \boldsymbol{\theta} = \beta \sum{i,j \in C(\boldsymbol{\theta}t)} \frac{| \mathbf{c}{ij} |}{1 + e^{-\gamma d{ij}}}$ ($\nabla L(\boldsymbol{\theta}t) \times \mathbf{c}{ij})$)
	\State Update parameters: ($\boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}_t - \alpha \nabla L(\boldsymbol{\theta}_t) + \Delta \boldsymbol{\theta}$)
	\State ($t \gets t + 1$)
\EndWhile
\end{algorithmic}
\end{algorithm}




This refined CGD algorithm uses a sigmoid function to modulate the influence of each chiral pair based on the topological distance \(d_{ij}\) between neurons. Chiral pairs that are topologically "closer" (smaller \(d_{ij}\)) have a stronger influence on the update. The magnitude of the chiral vector \(\| \mathbf{c}_{ij} \|\) also contributes, allowing pairs with greater asymmetry to exert more influence. The parameter \(\beta\) controls the global effect of chirality, while \(\gamma\) modulates the sigmoid's steepness, providing control over the sensitivity to topological distances \(d_{ij}\).



\section{Conclusion}
This research proposal presents a novel approach to gradient descent optimization that holds significant promise. By leveraging the power of chiral topologies and incorporating biologically plausible mechanisms into the optimization process, this research has the potential to overcome the limitations of traditional gradient descent and usher in a new era of more efficient and effective deep learning models. The research plan detailed above, if successfully executed, will provide valuable insight into the use of chiral gradient descent and pave the way for its deployment in real-world applications. The next steps will involve developing and testing the CGD algorithm, conducting rigorous experiments, and analyzing the findings to validate its performance and contribute to the advancement of deep learning methodologies.

\bibliographystyle{plain}
\bibliography{references} % Create a separate references.bib file for this




\end{document}

\documentclass[12pt, a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry} % Standard margin setup
\usepackage{amsmath, amsfonts, amssymb} % For math
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{graphicx} % For images
\usepackage{hyperref} % For hyperlinks
\usepackage{enumitem} % For better list control
\usepackage{abstract} % For abstract formatting
\usepackage{titlesec} % For title formatting
\usepackage{cite}

% Title formatting
\titleformat{\section}{\normalfont\Large\bfseries}{\thesection}{1em}{}
\titleformat{\subsection}{\normalfont\large\bfseries}{\thesubsection}{1em}{}

% Spacing
\setlength{\parskip}{1em}
\setlength{\parindent}{0em}

% Abstract spacing
\setlength{\absleftindent}{0mm}
\setlength{\absrightindent}{0mm}

% Title and author information
\title{\vspace{-2cm}\textbf{Research Proposal: Exploring Chiral Topologies for Enhanced Gradient Descent}}
\author{\textbf{Paul Lowndes} \\ \texttt{ZeroTrust@NSHkr.com}}
\date{\small December 4, 2024} % Smaller date formatting

\begin{document}

\maketitle
\vspace{-1.5em} % Adjust spacing after title

\begin{abstract}
This research proposal outlines a novel approach to gradient descent optimization, termed Chiral Gradient Descent (CGD), which incorporates topological information and rotational dynamics inspired by chirality in biological systems. The proposed methodology will investigate how chiral structures within neural networks can be leveraged to improve exploration of the parameter space, potentially leading to more robust and efficient training. The research will involve developing a mathematical framework for CGD, implementing the algorithm in a deep learning framework, and conducting experiments on benchmark datasets to evaluate CGD's performance compared to standard gradient descent methods. The expected outcomes include a refined mathematical formulation of CGD, an open-source implementation, and experimental results demonstrating the potential benefits of CGD for various machine learning tasks.
\end{abstract}

\section{Introduction}

Gradient descent is widely used for optimizing machine learning models, but its susceptibility to local minima remains a challenge. Chiral gradient descent (CGD) presents a novel approach, drawing inspiration from the concept of chirality. CGD aims to enhance standard gradient descent by introducing asymmetry into the update process, motivated by the observation that many biological systems, including neural networks in the brain, exhibit chiral structures. We hypothesize this could lead to more efficient exploration of the parameter space and faster, more stable convergence.

Gradient descent, while highly effective, suffers from limitations in complex landscapes, thus the exploration of alternative methods. Chiral Gradient Descent (CGD) introduces a novel mechanism by leveraging chiral asymmetry, inspired by natural systems where chirality plays a crucial role in dynamic interactions. By incorporating topological information and rotational dynamics, CGD aims to enhance the exploration capabilities of traditional gradient descent, potentially leading to faster and more robust convergence.  This builds on previous work exploring various gradient descent variants \cite{induraj2023variants}.

\section{Chiral Gradient Descent}

Chiral Gradient Descent (CGD) modifies the gradient update rule by incorporating chiral vectors, which introduce rotational dynamics into the optimization process. This approach is inspired by natural asymmetry observed in biological systems. The chiral term, incorporating a sigmoid function, allows for dynamic adjustments based on topological distances within the network, potentially enhancing exploration of the parameter space and leading to more robust convergence. This sigmoid function modulates the influence of each chiral pair based on the topological distance between neurons, allowing for local chiral effects to dominate while diminishing the impact of distant pairs. The mathematical formulation of CGD involves the cross product of the gradient with a chiral vector, adding a layer of complexity and potential to the optimization process.


\section{Applying Chiral Topologies}

We represent a neural network's topology as a graph \(G = (V, E)\). A chiral pair of neurons \((v_i, v_j)\) is defined based on topological asymmetry, such as differences in shortest path lengths from a common ancestor. For each chiral pair, a chiral vector \(\mathbf{c}_{ij}\) is defined, reflecting their topological relationship. Measures such as differences in path lengths, or other metrics relating to properties of chiral topologies will determine a numerical representation for relative asymmetry and help refine our distance metric between them. These metrics could involve other topological or information-theoretic properties like local or global curvature or the orientation or density of node or edge data.

\section{Mathematical Formulation}

The core innovation of CGD lies in its gradient calculation which incorporates chiral vectors:

\begin{equation} \label{eq:cgd_sigmoid}
\boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}_t - \alpha \nabla L(\boldsymbol{\theta}_t) + \beta \sum_{i,j \in C} s(w_{ij}, \mathbf{c}_{ij}) (\nabla L(\boldsymbol{\theta}_t) \times \mathbf{c}_{ij})
\end{equation}

Where:
\begin{itemize}
    \item \(\boldsymbol{\theta}_t\) represents the parameter vector at iteration \(t\).
    \item \(\alpha\) denotes the learning rate.
    \item \(\nabla L(\boldsymbol{\theta}_t)\) is the gradient of the loss function at iteration \(t\).
    \item \(\beta\) represents the chirality parameter, which modulates the influence of the chiral vectors.
    \item \(C\) denotes the set of chiral pairs being considered during the update step, which may vary at each iteration depending on the method or constraints being used by the researcher.
    \item \(w_{ij}\) represents a weight associated with the chiral pair \((i,j)\), and may reflect asymmetry measures related to properties of their chiral topologies.
    \item \(\mathbf{c}_{ij}\) represents the chiral vector for the chiral pair \((i, j)\).
    \item \(\times\) represents the cross-product.
    \item \(s(w_{ij},\mathbf{c}_{ij})\) is a function designed to blend the chiral vector's influence with considerations based on a weight \(w_{ij}\).
\end{itemize}

\section{Training Intuition and Higher Dimensions}
Understanding chirality in higher dimensions can begin with visualizing simple cases (2D and 3D), followed by mathematical generalization to 4D, 5D, and beyond. Training involves visualization, representation with vectors and matrices, implementing simple transformations, and generalizing to higher dimensions.

Understanding the role of chirality in higher dimensions requires a shift in perspective, as traditional geometric intuitions may not directly apply. By visualizing lower-dimensional cases and gradually extending these insights to higher dimensions, researchers can develop a deeper intuition for the impact of chiral dynamics on learning. The use of vector and matrix representations allows for the implementation of simple transformations that can be generalized, providing a framework for exploring the effects of chirality across various dimensional spaces.





\section{Towards CGD: A Synthesis}

Chiral Gradient Descent (CGD) aims to enhance the efficiency and robustness of standard gradient descent by incorporating chirality—a concept of asymmetry—into the optimization process. This asymmetry is inspired by the prevalence of chiral structures and functions in biological systems, suggesting that introducing similar principles in optimization algorithms could lead to advantages not seen in more traditional, gradient-based methods.

\subsection{The Chiral Update Rule}

The core innovation of CGD is its update rule:

\begin{equation} \label{eq:cgd_sigmoid_final}
\boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}t - \alpha \nabla L(\boldsymbol{\theta}t) + \beta \sum{i,j \in C(\boldsymbol{\theta}t)} \frac{| \mathbf{c}{ij} |}{1 + e^{-\gamma d{ij}}} (\nabla L(\boldsymbol{\theta}t) \times \mathbf{c}{ij})
\end{equation}

Where:




\begin{itemize}
    \item \(\boldsymbol{\theta}_t\): Parameter vector at time \(t\).
    \item \(\alpha\): Learning rate.
    \item \(\nabla L(\boldsymbol{\theta}_t)\): Gradient of the loss function.
    \item \(\beta\): Global chirality parameter.
    \item \(C(\boldsymbol{\theta}_t)\): Set of relevant chiral pairs, potentially changing dynamically during training.
    \item \(\mathbf{c}_{ij}\): Chiral vector for pair (i, j), representing the direction and magnitude of the chiral influence (calculated using methods detailed in Section 5).
    \item \(d_{ij}\): Topological distance between nodes i and j, reflecting their structural relationship. This could be shortest-path distance, a measure of graph centrality, or other relevant metrics, depending on the properties of the data.
    \item \(\gamma\): Parameter controlling the sigmoid function’s steepness, determining the sensitivity to topological distance. Larger values of \(\gamma\) result in a sharper transition in the sigmoid function, while smaller values result in a more gradual transition, providing a mechanism to adjust the influence of distance on the weights.
\end{itemize}

\subsection{Dynamic Chiral Pair Selection: ($C(\boldsymbol{\theta}_t))$}

The set of relevant chiral pairs, ($C(\boldsymbol{\theta}_t)$), is not static but dynamically determined at each iteration t. This selection process can be based on several factors:

\begin{itemize}
\item \textbf{Gradient Magnitude:} Prioritize pairs whose corresponding gradients exceed a certain threshold, focusing on areas of the network where learning is most active.
\item \textbf{Topological Distance:} Include pairs within a certain topological radius, preventing long-range chiral interactions from overwhelming the update.
\item \textbf{Asymmetry Score:} Incorporate an asymmetry score (e.g., based on the cosine similarity between the feature embeddings of the chiral pair, as discussed in Section 5). Select pairs whose asymmetry scores exceed a specific threshold, focusing on the most significant asymmetries within the network structure.
\item \textbf{Temporal Dynamics:} For recurrent networks, introduce a temporal component into the selection process, considering factors like previous activation patterns or temporal correlations.
\end{itemize}

\subsection{Biological Inspiration}

The sigmoid function in Equation \ref{eq:cgd_sigmoid_final} is inspired by the graded nature of synaptic weights in biological neural systems. The weight ($w_{ij}$) can be interpreted as reflecting the strength of the chiral interaction, analogous to synaptic efficacy. The sigmoid function ensures that the chiral term's influence decreases smoothly with increasing distance, mirroring how the influence of a neuron on its neighbors diminishes with physical distance in biological circuits.






 
\section{Identifying Chiral Pairs: A Topologically-Informed Approach}

This section details a novel method for identifying chiral pairs within complex networks, extending the approach described in Zhang et al. \cite{zhang2018machine} for identifying topological invariants.  Instead of directly predicting topological invariants, we adapt their convolutional neural network (CNN) architecture to identify pairs of nodes exhibiting chiral topological features, focusing on asymmetries within the network's structure and information flow.  This will form Phase 1 of our system for identifying chiral pairs to be used in subsequent phases to implement chiral gradient descent.

\subsection{Network Representation}

As in the previous sections, we represent networks as directed graphs $G = (V, E)$, where $V$ is the set of nodes and $E$ is the set of directed edges. Each edge $e_{ij} \in E$ connecting node $v_i$ to node $v_{j}$ has an associated weight $w_{ij}$ representing the strength of the connection (e.g., correlation between node activations, information flow, or interaction strength). We extend the graph representation by including additional node attributes that might influence the identification of chiral pairs, such as node centrality, community membership, and other topological measures that could prove useful in identifying pairs in the network.  This extended representation is richer and more nuanced compared to simple directed graphs and is necessary to capture the more complex relationships between nodes in the network.


As in the previous sections, we represent networks as directed graphs $G = (V, E)$, where $V$ is the set of nodes and $E$ is the set of directed edges.  Each edge $e_{ij} \in E$ connecting node $v_{i}$ to node $v$.

\subsection{Adapting the Convolutional Neural Network}

We adapt the CNN architecture proposed in Zhang et al. \cite{zhang2018machine}  (see Figure 1 in the original paper) to learn local topological features related to chirality. The input to the CNN will be a matrix representation of the local neighborhood around each node in the graph.  This representation will be constructed by including several elements:

Node Attributes:  Include node attributes such as centrality and community membership in the input matrix.
Edge Weights:  The edge weights from the node to its neighbors are added to the input matrix.
Shortest Path Lengths:  Compute the shortest path lengths between each pair of neighbors, which will inform the computation of the chiral vector in the subsequent phases.

This extended input representation captures both local topology and more global network features. This information is crucial for accurately identifying chiral pairs, unlike the approach in the original paper which only focused on computing the winding number.

\subsection{Chiral Pair Identification}

The output of the CNN is a vector that represents a topological feature embedding of the local neighborhood for each node.  We define a chiral pair as a pair of nodes whose topological feature embeddings show a high degree of asymmetry or anti-correlation. This asymmetry or anti-correlation is evaluated using a distance metric, such as cosine similarity. We select the top pairs that maximize the asymmetry as the chiral pairs relevant to performing CGD in subsequent phases.  The selection process could be made more complex to filter out pairs with specific traits or incorporate more data to reduce computational cost or improve performance.






\section{Identifying Chiral Pairs: A Topologically-Informed Approach}

This section details a method for identifying chiral pairs, building upon Zhang \textit{et al.} [1] but incorporating novel elements to capture asymmetry relevant to narrative structures and social networks. This forms Phase 1 of our system, providing the foundation for chiral gradient descent.

\subsection{Network Representation (Enhanced)}

We represent networks as directed graphs $G = (V, E)$ with weighted edges $w_{ij}$. Crucially, we augment this with \textbf{relative path information}. For each node $v_i$, we pre-compute:

\begin{itemize}
    \item \textbf{Shortest Path Lengths:} The shortest path length from $v_i$ to every other node $v_j$ (denoted $d(v_i, v_j)$).
    \item \textbf{Common Ancestors and Path Differences:} For each pair of nodes ($v_i, v_j$), we identify their common ancestors and calculate the \textit{difference} in path lengths from each common ancestor to $v_i$ and $v_j$. This captures the asymmetry in how $v_i$ and $v_j$ relate to their shared history or context.
\end{itemize}

This richer representation goes beyond simple connectivity, encoding structural asymmetries relevant to narratives (e.g., how different characters relate to key events) and social networks (e.g., differing perspectives on shared information).

\subsection{Convolutional Neural Network (Adapted)}

We adapt the CNN architecture from Zhang \textit{et al.} [1] to learn chiral features. The input to the CNN for node $v_i$ now includes:

\begin{itemize}
    \item \textbf{Local Neighborhood Structure:} Edge weights $w_{ij}$ for $v_j$ in $v_i$'s neighborhood.
    \item \textbf{Relative Path Information:} For each neighbor $v_j$, include the shortest path length $d(v_i, v_j)$ and the differences in path lengths from common ancestors.
\end{itemize}

This combined input allows the CNN to learn features sensitive to both local connectivity and global topological asymmetries.

\subsection{Chiral Pair Identification (Refined)}

Instead of simply using cosine distance, we define a more nuanced chirality score:

\begin{equation}
\text{ChiralScore}(v_i, v_j) = \text{Asymmetry}(F_i, F_j) \times \text{PathDifference}(v_i, v_j)
\end{equation}

Where:

\begin{itemize}
    \item $F_i, F_j$ are the feature embeddings from the CNN for nodes $v_i$ and $v_j$.
    \item $\text{Asymmetry}(F_i, F_j)$ measures the asymmetry between the embeddings (e.g., using cosine distance or a learned metric).
    \item $\text{PathDifference}(v_i, v_j)$ is a weighted average of the path length differences from common ancestors, emphasizing structural asymmetry.
\end{itemize}

Pairs with high $\text{ChiralScore}$ are identified as chiral pairs. This combined score captures both feature-level and structural asymmetry.

\subsection{Algorithm}

\begin{algorithm}[H]
\caption{Chiral Pair Identification}
\begin{algorithmic}[1]
\Require Graph $G=(V,E)$, CNN model, chirality threshold $\tau$
\State Pre-compute shortest path lengths and path differences for all node pairs
\State Initialize empty set of chiral pairs $C$
\For{each node $v_i \in V$}
    \State Construct input matrix $M_i$ (neighborhood, paths)
    \State $F_i \gets$ CNN($M_i$)
\EndFor
\For{each pair of nodes $(v_i, v_j) \in V \times V$}
    \State $S_{ij} \gets \text{ChiralScore}(v_i, v_j)$
    \If{$S_{ij} > \tau$}
        \State $C \gets C \cup \{(v_i, v_j)\}$
    \EndIf
\EndFor
\Return $C$
\end{algorithmic}
\end{algorithm}

\subsection{Discussion}

This method directly addresses the challenge of defining and detecting chirality relevant to narrative and social network analysis. By incorporating relative path information and a combined chirality score, it captures more nuanced asymmetries than simply comparing feature embeddings. The use of common ancestor paths adds a "historical" or "contextual" dimension to the chirality measure, which is particularly relevant for understanding how information and relationships evolve in narratives and social networks. The hierarchical application and the flexibility in defining the Asymmetry and PathDifference functions provide adaptability for various applications.






\subsection{Hierarchical Application}

To handle hierarchical structures, we apply the method recursively to sub-graphs within the network. This hierarchical application identifies chiral pairs at different scales, allowing for a more granular analysis of asymmetry in multi-level structures, which mirrors the multi-scale nature of hierarchical structures in narratives and social networks.


\subsection{Algorithm}


\begin{algorithm}[H]
\caption{Chiral Pair Identification}
\begin{algorithmic}[1]
\Require Graph $G=(V,E)$, CNN model, asymmetry threshold $\tau$
\State Initialize empty set of chiral pairs $C$
\For{each node $v_i \in V$}
    \State Construct input matrix $M_i$ for $v_i$'s neighborhood
    \State Obtain feature embedding vector $F_i$ from CNN($M_i$)
\EndFor
\For{each pair of nodes $(v_i, v_j) \in V \times V$}
    \State Compute asymmetry score $A_{ij} = d(F_i, F_j)$ (e.g., using cosine distance)
    \If{$A_{ij} > \tau$}
        \State Add $(v_i, v_j)$ to $C$
    \EndIf
\EndFor
\State \Return Set of chiral pairs $C$
\end{algorithmic}
\end{algorithm}


\subsection{Discussion}

This method builds upon the success of CNNs in learning complex patterns from local data and provides a rigorous approach to identify chiral pairs.  By incorporating topological features and hierarchical application, it addresses the complexities of analyzing asymmetries in real-world systems.  The choice of CNN architecture and asymmetry threshold requires careful evaluation and tuning for specific applications and datasets.  We will compare this approach with alternative methods for measuring graph similarity and asymmetry in our validation experiments.

 



\begin{equation} \label{eq:cgd_sigmoid_final}
\boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}_t - \alpha \nabla L(\boldsymbol{\theta}_t) + \beta \sum_{i,j \in C(\boldsymbol{\theta}_t)}  \frac{\| \mathbf{c}_{ij} \|}{1 + e^{-\gamma d_{ij}}} (\nabla L(\boldsymbol{\theta}_t) \times \mathbf{c}_{ij})
\end{equation}

Where:

\begin{itemize}
    \item \(\boldsymbol{\theta}_t\): Parameter vector at time \(t\).
    \item \(\alpha\): Learning rate.
    \item \(\nabla L(\boldsymbol{\theta}_t)\): Gradient of the loss function.
    \item \(\beta\): Global chirality parameter.
    \item \(C(\boldsymbol{\theta}_t)\): Set of relevant chiral pairs, potentially changing dynamically during training.
    \item \(\mathbf{c}_{ij}\): Chiral vector for pair (i, j), calculated based on topology.
    \item \(d_{ij}\): Topological distance between \(i\) and \(j\) based on features like difference in path lengths, curvature measures, node or edge distribution densities, etc.
    \item \(\gamma\): Scaling parameter for the sigmoid function, influencing the impact of \(d_{ij}\).
\end{itemize}

\begin{algorithm}
\caption{Chiral Gradient Descent (CGD)}
\label{alg:cgd}
\begin{algorithmic}
\Require Learning rate ($\alpha$), chirality parameter ($\beta$), scaling parameter ($\gamma$), initial parameters (\boldsymbol{\theta}0)
	\While{not converged}
	\State Compute gradient: ($\nabla L(\boldsymbol{\theta}t)$)
	\State Determine relevant chiral pairs: ($C(\boldsymbol{\theta}t)$) (using gradient magnitude, topological distance, asymmetry scores, and other factors)
	\State Calculate chiral vectors ($\mathbf{c}{ij}$) and distances ($d{ij}$) for ($(i, j) \in C(\boldsymbol{\theta}t)$)
	\State ($\Delta \boldsymbol{\theta} = \beta \sum{i,j \in C(\boldsymbol{\theta}t)} \frac{| \mathbf{c}{ij} |}{1 + e^{-\gamma d{ij}}}$ ($\nabla L(\boldsymbol{\theta}t) \times \mathbf{c}{ij})$)
	\State Update parameters: ($\boldsymbol{\theta}_{t+1} = \boldsymbol{\theta}_t - \alpha \nabla L(\boldsymbol{\theta}_t) + \Delta \boldsymbol{\theta}$)
	\State ($t \gets t + 1$)
\EndWhile
\end{algorithmic}
\end{algorithm}




This refined CGD algorithm uses a sigmoid function to modulate the influence of each chiral pair based on the topological distance \(d_{ij}\) between neurons. Chiral pairs that are topologically "closer" (smaller \(d_{ij}\)) have a stronger influence on the update. The magnitude of the chiral vector \(\| \mathbf{c}_{ij} \|\) also contributes, allowing pairs with greater asymmetry to exert more influence. The parameter \(\beta\) controls the global effect of chirality, while \(\gamma\) modulates the sigmoid's steepness, providing control over the sensitivity to topological distances \(d_{ij}\).



\section{Conclusion}
This research proposal presents a novel approach to gradient descent optimization that holds significant promise. By leveraging the power of chiral topologies and incorporating biologically plausible mechanisms into the optimization process, this research has the potential to overcome the limitations of traditional gradient descent and usher in a new era of more efficient and effective deep learning models. The research plan detailed above, if successfully executed, will provide valuable insight into the use of chiral gradient descent and pave the way for its deployment in real-world applications. The next steps will involve developing and testing the CGD algorithm, conducting rigorous experiments, and analyzing the findings to validate its performance and contribute to the advancement of deep learning methodologies.

\bibliographystyle{plain}
\bibliography{references} % Create a separate references.bib file for this




\end{document}

